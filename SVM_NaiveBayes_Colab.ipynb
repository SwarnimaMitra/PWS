{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e63f1d48",
      "metadata": {
        "id": "e63f1d48"
      },
      "source": [
        "# SVM Classifier on Iris Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q21. Write a Python program to train an SVM Classifier on the Iris dataset and evaluate accuracy:"
      ],
      "metadata": {
        "id": "6yTV_I39loSq"
      },
      "id": "6yTV_I39loSq"
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train SVM classifier\n",
        "clf = SVC(kernel='linear', random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = clf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Iris SVM Classifier Accuracy: {accuracy:.2f}')\n"
      ],
      "metadata": {
        "id": "g_Kp2wt8lwo5"
      },
      "id": "g_Kp2wt8lwo5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q22 Write a Python program to train two SVM classifiers with Linear and RBF kernels on the Wine dataset, then\n",
        "compare their accuracies:"
      ],
      "metadata": {
        "id": "DlwMV_gul4fN"
      },
      "id": "DlwMV_gul4fN"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Wine dataset\n",
        "wine = load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Linear kernel SVM\n",
        "linear_clf = SVC(kernel='linear', random_state=42)\n",
        "linear_clf.fit(X_train, y_train)\n",
        "linear_pred = linear_clf.predict(X_test)\n",
        "accuracy_linear = accuracy_score(y_test, linear_pred)\n",
        "\n",
        "# RBF kernel SVM\n",
        "rbf_clf = SVC(kernel='rbf', random_state=42)\n",
        "rbf_clf.fit(X_train, y_train)\n",
        "rbf_pred = rbf_clf.predict(X_test)\n",
        "accuracy_rbf = accuracy_score(y_test, rbf_pred)\n",
        "\n",
        "print(f\"SVM with Linear kernel Accuracy: {accuracy_linear:.2f}\")\n",
        "print(f\"SVM with RBF kernel Accuracy: {accuracy_rbf:.2f}\")\n"
      ],
      "metadata": {
        "id": "6d3Sy1Kll-jB"
      },
      "id": "6d3Sy1Kll-jB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q23: Write a Python program to train an SVM Regressor (SVR) on a housing dataset and evaluate it using Mean\n",
        "Squared Error (MSE):"
      ],
      "metadata": {
        "id": "gA54KcF6o6-W"
      },
      "id": "gA54KcF6o6-W"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the California housing dataset\n",
        "housing = fetch_california_housing()\n",
        "X, y = housing.data, housing.target\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train SVR\n",
        "svr = SVR()\n",
        "svr.fit(X_train, y_train)\n",
        "\n",
        "# Predict and calculate MSE\n",
        "y_pred = svr.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f'SVR Mean Squared Error: {mse:.2f}')\n",
        "\n"
      ],
      "metadata": {
        "id": "_M3C1OBQpH__"
      },
      "id": "_M3C1OBQpH__",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q24 Write a Python program to train an SVM Classifier with a Polynomial Kernel and visualize the decision\n",
        "boundary:"
      ],
      "metadata": {
        "id": "XtPWcCs-pO8O"
      },
      "id": "XtPWcCs-pO8O"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Load Iris: use only the first 2 features for 2D plotting\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data[:, :2]\n",
        "y = iris.target\n",
        "\n",
        "# Use only classes 0 and 1 for binary classification visual\n",
        "X = X[y != 2]\n",
        "y = y[y != 2]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
        "\n",
        "# Train polynomial kernel SVM\n",
        "clf = SVC(kernel='poly', degree=3, C=1)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Function to plot decision boundary\n",
        "def plot_decision_boundary(X, y, clf):\n",
        "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "        y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "            xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01), np.arange(y_min, y_max, 0.01))\n",
        "                Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "                    Z = Z.reshape(xx.shape)\n",
        "                        plt.contourf(xx, yy, Z, alpha=0.3)\n",
        "                            plt.scatter(X[:, 0], X[:, 1], c=y, edgecolor='k', s=30)\n",
        "                                plt.xlabel(iris.feature_names[0])\n",
        "                                    plt.ylabel(iris.feature_names[1])\n",
        "                                        plt.title('SVM with Polynomial Kernel')\n",
        "                                            plt.show()\n",
        "\n",
        "                                            plot_decision_boundary(X, y, clf)\n"
      ],
      "metadata": {
        "id": "Cp8x3JHHpWoN"
      },
      "id": "Cp8x3JHHpWoN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q25 Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer dataset and\n",
        "evaluate accuracy:"
      ],
      "metadata": {
        "id": "0ZHJGo8Qpbg5"
      },
      "id": "0ZHJGo8Qpbg5"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load data\n",
        "cancer = load_breast_cancer()\n",
        "X, y = cancer.data, cancer.target\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train GaussianNB\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "# Evaluate\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Gaussian NB Accuracy on Breast Cancer: {accuracy:.2f}')\n"
      ],
      "metadata": {
        "id": "1S_54s1tpgZX"
      },
      "id": "1S_54s1tpgZX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q26: Write a Python program to train a Multinomial Naïve Bayes classifier for text classification using the 20\n",
        "Newsgroups dataset."
      ],
      "metadata": {
        "id": "rdGm4DYapks0"
      },
      "id": "rdGm4DYapks0"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Fetch dataset\n",
        "data = fetch_20newsgroups(subset='all', shuffle=True, random_state=42)\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Vectorize\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "X_vec = vectorizer.fit_transform(X)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_vec, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train MultinomialNB\n",
        "clf = MultinomialNB()\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = clf.predict(X_test)\n",
        "print(f'MultinomialNB Accuracy on 20 Newsgroups: {accuracy_score(y_test, y_pred):.2f}')\n",
        "print(classification_report(y_test, y_pred, target_names=data.target_names))\n"
      ],
      "metadata": {
        "id": "dawx52iQpp_Q"
      },
      "id": "dawx52iQpp_Q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "27. Write a Python program to train an SVM Classifier with different C values and compare the decision\n",
        "boundaries visually="
      ],
      "metadata": {
        "id": "gvybSvJnvkCi"
      },
      "id": "gvybSvJnvkCi"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import svm, datasets\n",
        "\n",
        "# Load iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data[:, :2]  # First two features\n",
        "y = iris.target\n",
        "\n",
        "# Train SVMs with different C values\n",
        "C_values = [0.1, 1, 100]\n",
        "models = []\n",
        "for C in C_values:\n",
        "    model = svm.SVC(kernel='linear', C=C)\n",
        "        model.fit(X, y)\n",
        "            models.append(model)\n",
        "\n",
        "            # Plot decision boundaries\n",
        "            x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "            y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "            xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
        "                                 np.arange(y_min, y_max, 0.02))\n",
        "\n",
        "                                 plt.figure(figsize=(15, 5))\n",
        "                                 for i, (model, C) in enumerate(zip(models, C_values)):\n",
        "                                     plt.subplot(1, 3, i+1)\n",
        "                                         Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "                                             Z = Z.reshape(xx.shape)\n",
        "                                                 plt.contourf(xx, yy, Z, alpha=0.8)\n",
        "                                                     plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k')\n",
        "                                                         plt.title(f'SVM with C={C}')\n",
        "                                                         plt.show()"
      ],
      "metadata": {
        "id": "JgACNpcjvn5E"
      },
      "id": "JgACNpcjvn5E",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "28 Write a Python program to train a Bernoulli Naïve Bayes classifier for binary classification on a dataset with\n",
        "binary features="
      ],
      "metadata": {
        "id": "Pc15pka0vsG2"
      },
      "id": "Pc15pka0vsG2"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Create binary dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20,\n",
        "                          n_informative=15, n_classes=2,\n",
        "                                                    random_state=42)\n",
        "\n",
        "                                                    # Convert to binary features\n",
        "                                                    X = (X > 0).astype(int)\n",
        "\n",
        "                                                    # Split data\n",
        "                                                    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "                                                    # Train and evaluate\n",
        "                                                    bnb = BernoulliNB()\n",
        "                                                    bnb.fit(X_train, y_train)\n",
        "                                                    y_pred = bnb.predict(X_test)\n",
        "                                                    print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")"
      ],
      "metadata": {
        "id": "1pY24xWJvyAl"
      },
      "id": "1pY24xWJvyAl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "30. Write a Python program to apply feature scaling before training an SVM model and compare results with\n",
        "unscaled data?"
      ],
      "metadata": {
        "id": "zmfNk-dGv04q"
      },
      "id": "zmfNk-dGv04q"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load data\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "# Without scaling\n",
        "svm_unscaled = SVC(kernel='rbf')\n",
        "svm_unscaled.fit(X_train, y_train)\n",
        "print(f\"Unscaled Accuracy: {svm_unscaled.score(X_test, y_test):.4f}\")\n",
        "\n",
        "# With scaling\n",
        "svm_scaled = make_pipeline(StandardScaler(), SVC(kernel='rbf'))\n",
        "svm_scaled.fit(X_train, y_train)\n",
        "print(f\"Scaled Accuracy: {svm_scaled.score(X_test, y_test):.4f}\")"
      ],
      "metadata": {
        "id": "PJyEhTR7v7OY"
      },
      "id": "PJyEhTR7v7OY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "31.Write a Python program to train an SVM Classifier and use GridSearchCV to tune the hyperparameters (C,\n",
        "gamma, kernel)?"
      ],
      "metadata": {
        "id": "l3g7jP1Cv-Ux"
      },
      "id": "l3g7jP1Cv-Ux"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Create dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=10, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "# Without smoothing\n",
        "gnb = GaussianNB(var_smoothing=0)\n",
        "gnb.fit(X_train, y_train)\n",
        "print(f\"No smoothing accuracy: {gnb.score(X_test, y_test):.4f}\")\n",
        "\n",
        "# With smoothing\n",
        "gnb_smooth = GaussianNB(var_smoothing=1e-9)\n",
        "gnb_smooth.fit(X_train, y_train)\n",
        "print(f\"With smoothing accuracy: {gnb_smooth.score(X_test, y_test):.4f}\")"
      ],
      "metadata": {
        "id": "jvtcJsvKwGeP"
      },
      "id": "jvtcJsvKwGeP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "32. Write a Python program to train an SVM Classifier on an imbalanced dataset and apply class weighting and\n",
        "check it improve accuracy?"
      ],
      "metadata": {
        "id": "EgXS618jBtsp"
      },
      "id": "EgXS618jBtsp"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import svm, datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from imblearn.datasets import make_imbalance\n",
        "from collections import Counter\n",
        "\n",
        "# Load the iris dataset and make it imbalanced\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data[:, :2]  # Use first two features for visualization\n",
        "y = iris.target\n",
        "\n",
        "# Create an imbalanced version (class 0 will be minority)\n",
        "X, y = make_imbalance(iris.data, iris.target, sampling_strategy={0: 10, 1: 50, 2: 50})\n",
        "\n",
        "print(\"Class distribution:\", Counter(y))\n",
        "\n",
        "# Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train SVM without class weighting\n",
        "svm_unweighted = svm.SVC(kernel='linear', random_state=42)\n",
        "svm_unweighted.fit(X_train, y_train)\n",
        "y_pred_unweighted = svm_unweighted.predict(X_test)\n",
        "\n",
        "# Train SVM with class weighting (balanced)\n",
        "svm_weighted = svm.SVC(kernel='linear', class_weight='balanced', random_state=42)\n",
        "svm_weighted.fit(X_train, y_train)\n",
        "y_pred_weighted = svm_weighted.predict(X_test)\n",
        "\n",
        "# Calculate accuracies\n",
        "acc_unweighted = accuracy_score(y_test, y_pred_unweighted)\n",
        "acc_weighted = accuracy_score(y_test, y_pred_weighted)\n",
        "\n",
        "print(\"\\nUnweighted SVM Accuracy: {:.2f}%\".format(acc_unweighted * 100))\n",
        "print(\"Weighted SVM Accuracy: {:.2f}%\".format(acc_weighted * 100))\n",
        "\n",
        "# More detailed evaluation\n",
        "print(\"\\nUnweighted SVM Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_unweighted))\n",
        "\n",
        "print(\"\\nWeighted SVM Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_weighted))\n",
        "\n",
        "# Plot decision boundaries\n",
        "def plot_decision_boundary(clf, X, y, title):\n",
        "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "        y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "            xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
        "                                     np.arange(y_min, y_max, 0.02))\n",
        "\n",
        "                                             Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "                                                 Z = Z.reshape(xx.shape)\n",
        "\n",
        "                                                         plt.contourf(xx, yy, Z, alpha=0.8)\n",
        "                                                             plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k')\n",
        "                                                                 plt.title(title)\n",
        "                                                                     plt.xlabel(iris.feature_names[0])\n",
        "                                                                         plt.ylabel(iris.feature_names[1])\n",
        "\n",
        "                                                                         plt.figure(figsize=(12, 5))\n",
        "                                                                         plt.subplot(1, 2, 1)\n",
        "                                                                         plot_decision_boundary(svm_unweighted, X_train, y_train, \"Unweighted SVM\")\n",
        "\n",
        "                                                                         plt.subplot(1, 2, 2)\n",
        "                                                                         plot_decision_boundary(svm_weighted, X_train, y_train, \"Weighted SVM (Balanced)\")\n",
        "                                                                         plt.tight_layout()\n",
        "                                                                         plt.show()"
      ],
      "metadata": {
        "id": "VCkncFODCZGE"
      },
      "id": "VCkncFODCZGE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "33. Write a Python program to implement a Naïve Bayes classifier for spam detection using email data?"
      ],
      "metadata": {
        "id": "FbWhEPRECeSI"
      },
      "id": "FbWhEPRECeSI"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the dataset (replace with your email dataset)\n",
        "# Sample dataset format: two columns - 'text' and 'label' (0=ham, 1=spam)\n",
        "data = pd.read_csv('emails.csv')  # Replace with your dataset path\n",
        "\n",
        "# If using the built-in dataset from sklearn (alternative)\n",
        "# from sklearn.datasets import fetch_20newsgroups\n",
        "# categories = ['alt.atheism', 'soc.religion.christian']\n",
        "# data = fetch_20newsgroups(subset='train', categories=categories)\n",
        "# df = pd.DataFrame({'text': data.data, 'label': data.target})\n",
        "\n",
        "# Preprocessing\n",
        "data['text'] = data['text'].str.lower()  # Convert to lowercase\n",
        "data['text'] = data['text'].str.replace('[^\\w\\s]', '')  # Remove punctuation\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    data['text'], data['label'], test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "    # Vectorize the text data (convert to numerical features)\n",
        "    vectorizer = CountVectorizer(stop_words='english', max_features=5000)\n",
        "    X_train_vec = vectorizer.fit_transform(X_train)\n",
        "    X_test_vec = vectorizer.transform(X_test)\n",
        "\n",
        "    # Train Naïve Bayes classifier\n",
        "    nb_classifier = MultinomialNB()\n",
        "    nb_classifier.fit(X_train_vec, y_train)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = nb_classifier.predict(X_test_vec)\n",
        "\n",
        "    # Evaluate the model\n",
        "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "    # Confusion matrix visualization\n",
        "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=['Ham', 'Spam'], yticklabels=['Ham', 'Spam'])\n",
        "                plt.xlabel('Predicted')\n",
        "                plt.ylabel('Actual')\n",
        "                plt.title('Confusion Matrix for Spam Detection')\n",
        "                plt.show()\n",
        "\n",
        "                # Example of predicting new emails\n",
        "                new_emails = [\n",
        "                    \"Congratulations! You've won a $1000 prize! Click here to claim!\",\n",
        "                        \"Hi John, just checking in about our meeting tomorrow\",\n",
        "                            \"Your account has been compromised. Verify your details now!\"\n",
        "                            ]\n",
        "\n",
        "                            new_emails_vec = vectorizer.transform(new_emails)\n",
        "                            predictions = nb_classifier.predict(new_emails_vec)\n",
        "\n",
        "                            print(\"\\nSample Predictions:\")\n",
        "                            for email, pred in zip(new_emails, predictions):\n",
        "                                print(f\"\\nEmail: {email[:50]}...\")\n",
        "                                    print(\"Prediction:\", \"Spam\" if pred == 1 else \"Ham\")"
      ],
      "metadata": {
        "id": "mjaIgT9QC09m"
      },
      "id": "mjaIgT9QC09m",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "34. Write a Python program to train an SVM Classifier and a Naïve Bayes Classifier on the same dataset and\n",
        "compare their accuracy?"
      ],
      "metadata": {
        "id": "0jkDqpczC3Fy"
      },
      "id": "0jkDqpczC3Fy"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load dataset (using Breast Cancer Wisconsin dataset as example)\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "feature_names = data.feature_names\n",
        "target_names = data.target_names\n",
        "\n",
        "# Split data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize classifiers\n",
        "svm_classifier = SVC(kernel='linear', random_state=42)\n",
        "nb_classifier = GaussianNB()\n",
        "\n",
        "# Train classifiers\n",
        "print(\"Training SVM Classifier...\")\n",
        "svm_classifier.fit(X_train, y_train)\n",
        "\n",
        "print(\"Training Naïve Bayes Classifier...\")\n",
        "nb_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "svm_pred = svm_classifier.predict(X_test)\n",
        "nb_pred = nb_classifier.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "svm_accuracy = accuracy_score(y_test, svm_pred)\n",
        "nb_accuracy = accuracy_score(y_test, nb_pred)\n",
        "\n",
        "# Print results\n",
        "print(\"\\nClassifier Comparison:\")\n",
        "print(f\"SVM Accuracy: {svm_accuracy:.4f}\")\n",
        "print(f\"Naïve Bayes Accuracy: {nb_accuracy:.4f}\")\n",
        "\n",
        "# Detailed classification reports\n",
        "print(\"\\nSVM Classification Report:\")\n",
        "print(classification_report(y_test, svm_pred, target_names=target_names))\n",
        "\n",
        "print(\"\\nNaïve Bayes Classification Report:\")\n",
        "print(classification_report(y_test, nb_pred, target_names=target_names))\n",
        "\n",
        "# Visual comparison\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.bar(['SVM', 'Naïve Bayes'], [svm_accuracy, nb_accuracy], color=['blue', 'orange'])\n",
        "plt.ylim(0.8, 1.0)\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Classifier Accuracy Comparison')\n",
        "for i, v in enumerate([svm_accuracy, nb_accuracy]):\n",
        "    plt.text(i, v + 0.01, f\"{v:.4f}\", ha='center')\n",
        "    plt.show()\n",
        "\n",
        "    # Feature importance analysis (for SVM)\n",
        "    if hasattr(svm_classifier, 'coef_'):\n",
        "        print(\"\\nTop 5 Important Features (SVM):\")\n",
        "            coef = svm_classifier.coef_[0]\n",
        "                top_features = np.argsort(np.abs(coef))[-5:][::-1]\n",
        "                    for i in top_features:\n",
        "                            print(f\"{feature_names[i]}: {coef[i]:.4f}\")"
      ],
      "metadata": {
        "id": "HAi73TZ4DbTd"
      },
      "id": "HAi73TZ4DbTd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "35. Write a Python program to perform feature selection before training a Naïve Bayes classifier and compare\n",
        "results?"
      ],
      "metadata": {
        "id": "5SYTUuTaDcaX"
      },
      "id": "5SYTUuTaDcaX"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Split data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Function to train and evaluate Naïve Bayes\n",
        "def evaluate_nb(X_train, X_test, y_train, y_test, title):\n",
        "    nb = GaussianNB()\n",
        "        nb.fit(X_train, y_train)\n",
        "            y_pred = nb.predict(X_test)\n",
        "                accuracy = accuracy_score(y_test, y_pred)\n",
        "                    print(f\"\\n{title} Results:\")\n",
        "                        print(f\"Accuracy: {accuracy:.4f}\")\n",
        "                            print(\"Classification Report:\")\n",
        "                                print(classification_report(y_test, y_pred))\n",
        "                                    return accuracy, nb\n",
        "\n",
        "                                    # 1. Train without feature selection\n",
        "                                    print(\"=== Naïve Bayes without Feature Selection ===\")\n",
        "                                    acc_full, nb_full = evaluate_nb(X_train, X_test, y_train, y_test, \"All Features\")\n",
        "\n",
        "                                    # 2. Perform feature selection using ANOVA F-value\n",
        "                                    print(\"\\n=== Performing Feature Selection ===\")\n",
        "                                    selector = SelectKBest(score_func=f_classif, k=5)  # Select top 5 features\n",
        "                                    X_train_selected = selector.fit_transform(X_train, y_train)\n",
        "                                    X_test_selected = selector.transform(X_test)\n",
        "\n",
        "                                    # Get selected feature names and scores\n",
        "                                    selected_features = feature_names[selector.get_support()]\n",
        "                                    feature_scores = selector.scores_[selector.get_support()]\n",
        "\n",
        "                                    print(\"\\nSelected Features:\")\n",
        "                                    for feature, score in zip(selected_features, feature_scores):\n",
        "                                        print(f\"{feature}: {score:.2f}\")\n",
        "\n",
        "                                        # 3. Train with selected features\n",
        "                                        print(\"\\n=== Naïve Bayes with Feature Selection ===\")\n",
        "                                        acc_selected, nb_selected = evaluate_nb(X_train_selected, X_test_selected, y_train, y_test, \"Selected Features\")\n",
        "\n",
        "                                        # Visual comparison\n",
        "                                        plt.figure(figsize=(10, 5))\n",
        "                                        plt.bar(['All Features', 'Selected Features'], [acc_full, acc_selected], color=['blue', 'green'])\n",
        "                                        plt.ylim(0.8, 1.0)\n",
        "                                        plt.ylabel('Accuracy')\n",
        "                                        plt.title('Naïve Bayes Performance: With vs Without Feature Selection')\n",
        "                                        for i, v in enumerate([acc_full, acc_selected]):\n",
        "                                            plt.text(i, v + 0.01, f\"{v:.4f}\", ha='center')\n",
        "                                            plt.show()\n",
        "\n",
        "                                            # Feature importance visualization\n",
        "                                            plt.figure(figsize=(12, 6))\n",
        "                                            plt.barh(range(len(selected_features)), feature_scores, color='purple')\n",
        "                                            plt.yticks(range(len(selected_features)), selected_features)\n",
        "                                            plt.xlabel('ANOVA F-value Score')\n",
        "                                            plt.title('Top Selected Features and Their Importance Scores')\n",
        "                                            plt.tight_layout()\n",
        "                                            plt.show()"
      ],
      "metadata": {
        "id": "8KskJaTnDzAP"
      },
      "id": "8KskJaTnDzAP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "36 Write a Python program to train an SVM Classifier using One-vs-Rest (OvR) and One-vs-One (OvO)\n",
        "strategies on the Wine dataset and compare their accuracy?"
      ],
      "metadata": {
        "id": "0l2IO5QCD1Du"
      },
      "id": "0l2IO5QCD1Du"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the Wine dataset\n",
        "wine = datasets.load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "feature_names = wine.feature_names\n",
        "target_names = wine.target_names\n",
        "\n",
        "# Preprocess the data\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize SVM with linear kernel\n",
        "svm = SVC(kernel='linear', random_state=42)\n",
        "\n",
        "# Create OvR and OvO classifiers\n",
        "ovr_classifier = OneVsRestClassifier(svm)\n",
        "ovo_classifier = OneVsOneClassifier(svm)\n",
        "\n",
        "# Train the classifiers\n",
        "print(\"Training One-vs-Rest SVM...\")\n",
        "ovr_classifier.fit(X_train, y_train)\n",
        "\n",
        "print(\"Training One-vs-One SVM...\")\n",
        "ovo_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "ovr_pred = ovr_classifier.predict(X_test)\n",
        "ovo_pred = ovo_classifier.predict(X_test)\n",
        "\n",
        "# Calculate accuracies\n",
        "ovr_accuracy = accuracy_score(y_test, ovr_pred)\n",
        "ovo_accuracy = accuracy_score(y_test, ovo_pred)\n",
        "\n",
        "# Print results\n",
        "print(\"\\nClassifier Comparison:\")\n",
        "print(f\"One-vs-Rest Accuracy: {ovr_accuracy:.4f}\")\n",
        "print(f\"One-vs-One Accuracy: {ovo_accuracy:.4f}\")\n",
        "\n",
        "# Detailed classification reports\n",
        "print(\"\\nOne-vs-Rest Classification Report:\")\n",
        "print(classification_report(y_test, ovr_pred, target_names=target_names))\n",
        "\n",
        "print(\"\\nOne-vs-One Classification Report:\")\n",
        "print(classification_report(y_test, ovo_pred, target_names=target_names))\n",
        "\n",
        "# Visual comparison\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.bar(['One-vs-Rest', 'One-vs-One'], [ovr_accuracy, ovo_accuracy], color=['blue', 'orange'])\n",
        "plt.ylim(0.7, 1.0)\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('SVM Multiclass Strategies Accuracy Comparison')\n",
        "for i, v in enumerate([ovr_accuracy, ovo_accuracy]):\n",
        "    plt.text(i, v + 0.01, f\"{v:.4f}\", ha='center')\n",
        "    plt.show()\n",
        "\n",
        "    # Number of classifiers created\n",
        "    print(f\"\\nNumber of classifiers created:\")\n",
        "    print(f\"OvR: {len(ovr_classifier.estimators_)} (equal to number of classes)\")\n",
        "    print(f\"OvO: {len(ovo_classifier.estimators_)} (n_classes * (n_classes - 1) / 2)\")"
      ],
      "metadata": {
        "id": "Mk3y5PJoD9CG"
      },
      "id": "Mk3y5PJoD9CG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "37.Write a Python program to train an SVM Classifier using Linear, Polynomial, and RBF kernels on the Breast\n",
        "Cancer dataset and compare their accuracy?"
      ],
      "metadata": {
        "id": "bDbFVfYJEKiI"
      },
      "id": "bDbFVfYJEKiI"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "cancer = datasets.load_breast_cancer()\n",
        "X = cancer.data\n",
        "y = cancer.target\n",
        "feature_names = cancer.feature_names\n",
        "target_names = cancer.target_names\n",
        "\n",
        "# Preprocess the data\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize SVM classifiers with different kernels\n",
        "svm_linear = SVC(kernel='linear', random_state=42)\n",
        "svm_poly = SVC(kernel='poly', degree=3, random_state=42)  # 3rd degree polynomial\n",
        "svm_rbf = SVC(kernel='rbf', random_state=42)  # Radial Basis Function\n",
        "\n",
        "# Train the classifiers\n",
        "print(\"Training Linear Kernel SVM...\")\n",
        "svm_linear.fit(X_train, y_train)\n",
        "\n",
        "print(\"Training Polynomial Kernel SVM...\")\n",
        "svm_poly.fit(X_train, y_train)\n",
        "\n",
        "print(\"Training RBF Kernel SVM...\")\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "linear_pred = svm_linear.predict(X_test)\n",
        "poly_pred = svm_poly.predict(X_test)\n",
        "rbf_pred = svm_rbf.predict(X_test)\n",
        "\n",
        "# Calculate accuracies\n",
        "linear_acc = accuracy_score(y_test, linear_pred)\n",
        "poly_acc = accuracy_score(y_test, poly_pred)\n",
        "rbf_acc = accuracy_score(y_test, rbf_pred)\n",
        "\n",
        "# Print results\n",
        "print(\"\\nKernel Comparison:\")\n",
        "print(f\"Linear Kernel Accuracy: {linear_acc:.4f}\")\n",
        "print(f\"Polynomial Kernel Accuracy: {poly_acc:.4f}\")\n",
        "print(f\"RBF Kernel Accuracy: {rbf_acc:.4f}\")\n",
        "\n",
        "# Detailed classification reports\n",
        "print(\"\\nLinear Kernel Classification Report:\")\n",
        "print(classification_report(y_test, linear_pred, target_names=target_names))\n",
        "\n",
        "print(\"\\nPolynomial Kernel Classification Report:\")\n",
        "print(classification_report(y_test, poly_pred, target_names=target_names))\n",
        "\n",
        "print(\"\\nRBF Kernel Classification Report:\")\n",
        "print(classification_report(y_test, rbf_pred, target_names=target_names))\n",
        "\n",
        "# Visual comparison\n",
        "plt.figure(figsize=(10, 6))\n",
        "kernels = ['Linear', 'Polynomial', 'RBF']\n",
        "accuracies = [linear_acc, poly_acc, rbf_acc]\n",
        "colors = ['blue', 'green', 'red']\n",
        "\n",
        "bars = plt.bar(kernels, accuracies, color=colors)\n",
        "plt.ylim(0.85, 1.0)\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('SVM Kernel Performance Comparison on Breast Cancer Dataset')\n",
        "\n",
        "# Add accuracy values on top of bars\n",
        "for bar in bars:\n",
        "    height = bar.get_height()\n",
        "        plt.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                     f'{height:.4f}',\n",
        "                                  ha='center', va='bottom')\n",
        "\n",
        "                                  plt.show()\n",
        "\n",
        "                                  # Feature importance for linear kernel (coefficients)\n",
        "                                  if hasattr(svm_linear, 'coef_'):\n",
        "                                      print(\"\\nTop 5 Important Features (Linear Kernel):\")\n",
        "                                          coef = svm_linear.coef_[0]\n",
        "                                              top_features = np.argsort(np.abs(coef))[-5:][::-1]\n",
        "                                                  for i in top_features:\n",
        "                                                          print(f\"{feature_names[i]}: {coef[i]:.4f}\")"
      ],
      "metadata": {
        "id": "0ZMUIa6VEPPZ"
      },
      "id": "0ZMUIa6VEPPZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "38 Write a Python program to train an SVM Classifier using Stratified K-Fold Cross-Validation and compute the\n",
        "average accuracy?"
      ],
      "metadata": {
        "id": "bstfu3P1EoU2"
      },
      "id": "bstfu3P1EoU2"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "cancer = datasets.load_breast_cancer()\n",
        "X = cancer.data\n",
        "y = cancer.target\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Initialize SVM classifier\n",
        "svm = SVC(kernel='rbf', random_state=42)\n",
        "\n",
        "# Set up Stratified K-Fold Cross-Validation\n",
        "n_splits = 5\n",
        "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "# Store accuracy scores\n",
        "accuracies = []\n",
        "fold_num = 1\n",
        "\n",
        "print(f\"Performing {n_splits}-Fold Stratified Cross-Validation...\\n\")\n",
        "\n",
        "# Perform cross-validation\n",
        "for train_index, test_index in skf.split(X, y):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "        y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "                # Train SVM\n",
        "                    svm.fit(X_train, y_train)\n",
        "\n",
        "                            # Make predictions and calculate accuracy\n",
        "                                y_pred = svm.predict(X_test)\n",
        "                                    acc = accuracy_score(y_test, y_pred)\n",
        "                                        accuracies.append(acc)\n",
        "\n",
        "                                                print(f\"Fold {fold_num} Accuracy: {acc:.4f}\")\n",
        "                                                    fold_num += 1\n",
        "\n",
        "                                                    # Calculate average accuracy\n",
        "                                                    avg_accuracy = np.mean(accuracies)\n",
        "                                                    std_accuracy = np.std(accuracies)\n",
        "\n",
        "                                                    print(\"\\nCross-Validation Results:\")\n",
        "                                                    print(f\"Average Accuracy: {avg_accuracy:.4f}\")\n",
        "                                                    print(f\"Standard Deviation: {std_accuracy:.4f}\")\n",
        "\n",
        "                                                    # Visualize fold accuracies\n",
        "                                                    plt.figure(figsize=(10, 5))\n",
        "                                                    plt.bar(range(1, n_splits+1), accuracies, color='skyblue')\n",
        "                                                    plt.axhline(y=avg_accuracy, color='r', linestyle='--', label=f'Average Accuracy: {avg_accuracy:.4f}')\n",
        "                                                    plt.xlabel('Fold Number')\n",
        "                                                    plt.ylabel('Accuracy')\n",
        "                                                    plt.title('SVM Classifier Accuracy per Fold (Stratified K-Fold CV)')\n",
        "                                                    plt.ylim(0.9, 1.0)\n",
        "                                                    plt.legend()\n",
        "                                                    plt.grid(True, linestyle='--', alpha=0.7)\n",
        "\n",
        "                                                    # Add accuracy values on top of bars\n",
        "                                                    for i, acc in enumerate(accuracies):\n",
        "                                                        plt.text(i+1, acc + 0.005, f\"{acc:.4f}\", ha='center')\n",
        "\n",
        "                                                        plt.show()"
      ],
      "metadata": {
        "id": "6L1RQBNjEwjv"
      },
      "id": "6L1RQBNjEwjv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "39 Write a Python program to train a Naïve Bayes classifier using different prior probabilities and compare\n",
        "performance?"
      ],
      "metadata": {
        "id": "T-J8G75HFEX2"
      },
      "id": "T-J8G75HFEX2"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "class_names = data.target_names\n",
        "class_counts = np.bincount(y)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define different priors to test\n",
        "priors_list = [\n",
        "    None,  # Let the model estimate from data\n",
        "        [0.5, 0.5],  # Equal priors\n",
        "            [0.3, 0.7],  # Favor class 1\n",
        "                [0.7, 0.3],  # Favor class 0\n",
        "                    [class_counts[0]/len(y), class_counts[1]/len(y)]  # Empirical priors\n",
        "                    ]\n",
        "\n",
        "                    prior_names = [\n",
        "                        \"Estimated from data\",\n",
        "                            \"Equal [0.5, 0.5]\",\n",
        "                                \"Favor class 1 [0.3, 0.7]\",\n",
        "                                    \"Favor class 0 [0.7, 0.3]\",\n",
        "                                        \"Empirical priors\"\n",
        "                                        ]\n",
        "\n",
        "                                        # Store results\n",
        "                                        results = []\n",
        "\n",
        "                                        # Train and evaluate with different priors\n",
        "                                        for prior, name in zip(priors_list, prior_names):\n",
        "                                            # Create and train model\n",
        "                                                nb = GaussianNB(priors=prior)\n",
        "                                                    nb.fit(X_train, y_train)\n",
        "\n",
        "                                                            # Make predictions\n",
        "                                                                y_pred = nb.predict(X_test)\n",
        "                                                                    y_proba = nb.predict_proba(X_test)[:, 1]\n",
        "\n",
        "                                                                            # Calculate metrics\n",
        "                                                                                acc = accuracy_score(y_test, y_pred)\n",
        "                                                                                    f1 = f1_score(y_test, y_pred)\n",
        "                                                                                        roc_auc = roc_auc_score(y_test, y_proba)\n",
        "\n",
        "                                                                                                # Store results\n",
        "                                                                                                    results.append({\n",
        "                                                                                                            'prior': name,\n",
        "                                                                                                                    'accuracy': acc,\n",
        "                                                                                                                            'f1_score': f1,\n",
        "                                                                                                                                    'roc_auc': roc_auc,\n",
        "                                                                                                                                            'actual_priors': nb.class_prior_  # The actual priors used\n",
        "                                                                                                                                                })\n",
        "\n",
        "                                                                                                                                                        print(f\"\\nResults for {name}:\")\n",
        "                                                                                                                                                            print(f\"Accuracy: {acc:.4f}\")\n",
        "                                                                                                                                                                print(f\"F1 Score: {f1:.4f}\")\n",
        "                                                                                                                                                                    print(f\"ROC AUC: {roc_auc:.4f}\")\n",
        "                                                                                                                                                                        print(f\"Actual priors used: {np.round(nb.class_prior_, 4)}\")\n",
        "\n",
        "                                                                                                                                                                        # Convert results to DataFrame for better visualization\n",
        "                                                                                                                                                                        import pandas as pd\n",
        "                                                                                                                                                                        results_df = pd.DataFrame(results)\n",
        "                                                                                                                                                                        print(\"\\nSummary Table:\")\n",
        "                                                                                                                                                                        print(results_df[['prior', 'accuracy', 'f1_score', 'roc_auc']])\n",
        "\n",
        "                                                                                                                                                                        # Plot comparison\n",
        "                                                                                                                                                                        metrics = ['accuracy', 'f1_score', 'roc_auc']\n",
        "                                                                                                                                                                        x = np.arange(len(prior_names))\n",
        "                                                                                                                                                                        width = 0.25\n",
        "\n",
        "                                                                                                                                                                        plt.figure(figsize=(12, 6))\n",
        "                                                                                                                                                                        for i, metric in enumerate(metrics):\n",
        "                                                                                                                                                                            plt.bar(x + i*width, results_df[metric], width, label=metric)\n",
        "\n",
        "                                                                                                                                                                                plt.xlabel('Prior Probabilities')\n",
        "                                                                                                                                                                                plt.ylabel('Score')\n",
        "                                                                                                                                                                                plt.title('Naïve Bayes Performance with Different Priors')\n",
        "                                                                                                                                                                                plt.xticks(x + width, prior_names, rotation=15, ha='right')\n",
        "                                                                                                                                                                                plt.ylim(0.8, 1.0)\n",
        "                                                                                                                                                                                plt.legend()\n",
        "                                                                                                                                                                                plt.grid(True, linestyle='--', alpha=0.7)\n",
        "                                                                                                                                                                                plt.tight_layout()\n",
        "                                                                                                                                                                                plt.show()\n",
        "\n",
        "                                                                                                                                                                                # Show actual class distribution\n",
        "                                                                                                                                                                                plt.figure(figsize=(6, 4))\n",
        "                                                                                                                                                                                plt.bar(class_names, class_counts, color=['skyblue', 'lightcoral'])\n",
        "                                                                                                                                                                                plt.title('Actual Class Distribution in Dataset')\n",
        "                                                                                                                                                                                plt.ylabel('Count')\n",
        "                                                                                                                                                                                plt.show()"
      ],
      "metadata": {
        "id": "NcGEg7wZFcbg"
      },
      "id": "NcGEg7wZFcbg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "40 Write a Python program to perform Recursive Feature Elimination (RFE) before training an SVM Classifier and\n",
        "compare accurac?"
      ],
      "metadata": {
        "id": "zlwajSTNFm6z"
      },
      "id": "zlwajSTNFm6z"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create preprocessing pipeline\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# 1. Train baseline SVM with all features\n",
        "svm_full = SVC(kernel='linear', random_state=42)\n",
        "svm_full.fit(X_train_scaled, y_train)\n",
        "y_pred_full = svm_full.predict(X_test_scaled)\n",
        "acc_full = accuracy_score(y_test, y_pred_full)\n",
        "\n",
        "# 2. Perform RFE\n",
        "print(\"Performing Recursive Feature Elimination...\")\n",
        "n_features_to_select = 10  # Select top 10 features\n",
        "svm_rfe = SVC(kernel='linear', random_state=42)\n",
        "rfe = RFE(estimator=svm_rfe, n_features_to_select=n_features_to_select, step=1)\n",
        "rfe.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Get selected features\n",
        "selected_features = np.where(rfe.support_)[0]\n",
        "print(\"\\nSelected Features ({} out of {}):\".format(n_features_to_select, X.shape[1]))\n",
        "for i, idx in enumerate(selected_features):\n",
        "    print(f\"{i+1}. {feature_names[idx]}\")\n",
        "\n",
        "    # Train SVM with selected features\n",
        "    X_train_selected = rfe.transform(X_train_scaled)\n",
        "    X_test_selected = rfe.transform(X_test_scaled)\n",
        "\n",
        "    svm_selected = SVC(kernel='linear', random_state=42)\n",
        "    svm_selected.fit(X_train_selected, y_train)\n",
        "    y_pred_selected = svm_selected.predict(X_test_selected)\n",
        "    acc_selected = accuracy_score(y_test, y_pred_selected)\n",
        "\n",
        "    # Compare results\n",
        "    print(\"\\nModel Comparison:\")\n",
        "    print(f\"Full Feature Set Accuracy ({X.shape[1]} features): {acc_full:.4f}\")\n",
        "    print(f\"Selected Features Accuracy ({n_features_to_select} features): {acc_selected:.4f}\")\n",
        "\n",
        "    print(\"\\nFull Feature Set Classification Report:\")\n",
        "    print(classification_report(y_test, y_pred_full))\n",
        "\n",
        "    print(\"\\nSelected Features Classification Report:\")\n",
        "    print(classification_report(y_test, y_pred_selected))\n",
        "\n",
        "    # Visual comparison\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.bar(['All Features', f'Selected {n_features_to_select} Features'],\n",
        "            [acc_full, acc_selected], color=['blue', 'green'])\n",
        "            plt.ylim(0.8, 1.0)\n",
        "            plt.ylabel('Accuracy')\n",
        "            plt.title('SVM Performance: Before vs After RFE Feature Selection')\n",
        "            for i, v in enumerate([acc_full, acc_selected]):\n",
        "                plt.text(i, v + 0.01, f\"{v:.4f}\", ha='center')\n",
        "                plt.show()\n",
        "\n",
        "                # Plot feature rankings\n",
        "                plt.figure(figsize=(12, 6))\n",
        "                ranking = rfe.ranking_\n",
        "                plt.barh(range(len(feature_names)), ranking, tick_label=feature_names)\n",
        "                plt.title('Feature Rankings (1 = selected)')\n",
        "                plt.xlabel('Ranking')\n",
        "                plt.tight_layout()\n",
        "                plt.show()"
      ],
      "metadata": {
        "id": "5p59SMjsFsV9"
      },
      "id": "5p59SMjsFsV9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "41. Write a Python program to train an SVM Classifier and evaluate its performance using Precision, Recall, and\n",
        "F1-Score instead of accuracy?"
      ],
      "metadata": {
        "id": "__ZZ3Po6GEIl"
      },
      "id": "__ZZ3Po6GEIl"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = datasets.load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "target_names = data.target_names\n",
        "\n",
        "# Preprocess the data\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize and train SVM classifier\n",
        "svm = SVC(kernel='rbf', random_state=42)\n",
        "svm.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = svm.predict(X_test)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "# Print metrics\n",
        "print(\"SVM Classifier Performance Metrics:\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-Score: {f1:.4f}\")\n",
        "\n",
        "# Detailed classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=target_names))\n",
        "\n",
        "# Confusion matrix visualization\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=target_names, yticklabels=target_names)\n",
        "            plt.xlabel('Predicted')\n",
        "            plt.ylabel('Actual')\n",
        "            plt.title('Confusion Matrix')\n",
        "            plt.show()\n",
        "\n",
        "            # Metrics comparison visualization\n",
        "            metrics = ['Precision', 'Recall', 'F1-Score']\n",
        "            values = [precision, recall, f1]\n",
        "\n",
        "            plt.figure(figsize=(8, 5))\n",
        "            bars = plt.bar(metrics, values, color=['blue', 'green', 'red'])\n",
        "            plt.ylim(0, 1.1)\n",
        "            plt.title('SVM Classifier Performance Metrics')\n",
        "            plt.ylabel('Score')\n",
        "\n",
        "            # Add values on top of bars\n",
        "            for bar in bars:\n",
        "                height = bar.get_height()\n",
        "                    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                                 f'{height:.4f}',\n",
        "                                              ha='center', va='bottom')\n",
        "\n",
        "                                              plt.show()"
      ],
      "metadata": {
        "id": "W2_zoyKMGU02"
      },
      "id": "W2_zoyKMGU02",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "42. Write a Python program to train a Naïve Bayes Classifier and evaluate its performance using Log Loss\n",
        "(Cross-Entropy Loss)?"
      ],
      "metadata": {
        "id": "_QQW8wTDGWOr"
      },
      "id": "_QQW8wTDGWOr"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import log_loss, accuracy_score, confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import seaborn as sns\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "feature_names = data.feature_names\n",
        "target_names = data.target_names\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Standardize features (important for probability calibration)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Initialize and train Naïve Bayes classifier\n",
        "nb_classifier = GaussianNB()\n",
        "nb_classifier.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Get predicted probabilities and class predictions\n",
        "y_pred_proba = nb_classifier.predict_proba(X_test_scaled)\n",
        "y_pred = nb_classifier.predict(X_test_scaled)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "logloss = log_loss(y_test, y_pred_proba)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Naïve Bayes Classifier Evaluation:\")\n",
        "print(f\"Log Loss (Cross-Entropy): {logloss:.4f}\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Plot predicted probabilities distribution\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(y_pred_proba[y_test == 0][:, 1], bins=30, alpha=0.5, label='Class 0 (Benign)', color='blue')\n",
        "plt.hist(y_pred_proba[y_test == 1][:, 1], bins=30, alpha=0.5, label='Class 1 (Malignant)', color='red')\n",
        "plt.xlabel('Predicted Probability for Class 1')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Distribution of Predicted Probabilities')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Confusion matrix\n",
        "conf_mat = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=target_names, yticklabels=target_names)\n",
        "            plt.xlabel('Predicted')\n",
        "            plt.ylabel('Actual')\n",
        "            plt.title('Confusion Matrix')\n",
        "            plt.show()\n",
        "\n",
        "            # Print probability calibration metrics\n",
        "            print(\"\\nProbability Calibration:\")\n",
        "            print(\"For well-calibrated classifiers, log loss should be close to 0.5-1.0 for binary classification\")\n",
        "            print(f\"Our Log Loss: {logloss:.4f}\")\n",
        "\n",
        "            # Additional metrics\n",
        "            from sklearn.metrics import classification_report\n",
        "            print(\"\\nClassification Report:\")\n",
        "            print(classification_report(y_test, y_pred, target_names=target_names))\n",
        "\n",
        "            # Plot log loss components (for understanding)\n",
        "            def log_loss_components(y_true, y_pred_proba):\n",
        "                eps = 1e-15  # to avoid log(0)\n",
        "                    y_pred_proba = np.clip(y_pred_proba, eps, 1 - eps)\n",
        "                        return -np.mean(y_true * np.log(y_pred_proba[:, 1]) +\n",
        "                                   (1 - y_true) * np.log(1 - y_pred_proba[:, 1]))\n",
        "\n",
        "                                   ll_components = log_loss_components(y_test, y_pred_proba)\n",
        "                                   print(f\"\\nLog Loss Components (Positive/Negative class contributions): {ll_components:.4f}\")"
      ],
      "metadata": {
        "id": "596A5d-8Gf8O"
      },
      "id": "596A5d-8Gf8O",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "43. Write a Python program to train an SVM Classifier and visualize the Confusion Matrix using seaborn?"
      ],
      "metadata": {
        "id": "vU7WKqz3GyeH"
      },
      "id": "vU7WKqz3GyeH"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn import datasets\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = datasets.load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "target_names = data.target_names\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Standardize features (important for SVM)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Initialize and train SVM classifier\n",
        "svm = SVC(kernel='rbf', random_state=42)\n",
        "svm.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = svm.predict(X_test)\n",
        "\n",
        "# Generate confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Create a beautiful confusion matrix visualization\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.set(font_scale=1.2)  # Adjust font size\n",
        "heatmap = sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            cbar=False, linewidths=0.5, linecolor='gray',\n",
        "                        xticklabels=target_names, yticklabels=target_names)\n",
        "\n",
        "                        # Customize the plot\n",
        "                        plt.title('SVM Classifier Confusion Matrix', pad=20, fontsize=16)\n",
        "                        plt.xlabel('Predicted Label', fontsize=14)\n",
        "                        plt.ylabel('True Label', fontsize=14)\n",
        "                        heatmap.set_yticklabels(heatmap.get_yticklabels(), rotation=0)  # Keep y-axis labels horizontal\n",
        "\n",
        "                        # Add accuracy and other metrics to the plot\n",
        "                        accuracy = np.trace(cm) / np.sum(cm)\n",
        "                        plt.text(0.5, -0.25,\n",
        "                                 f'Accuracy: {accuracy:.2%}\\n\\n{classification_report(y_test, y_pred, target_names=target_names)}',\n",
        "                                          ha='center', va='center', transform=plt.gca().transAxes,\n",
        "                                                   bbox=dict(facecolor='white', alpha=0.8))\n",
        "\n",
        "                                                   plt.tight_layout()\n",
        "                                                   plt.show()\n",
        "\n",
        "                                                   # Print classification report in console\n",
        "                                                   print(\"Classification Report:\")\n",
        "                                                   print(classification_report(y_test, y_pred, target_names=target_names))\n",
        "\n",
        "                                                   # Additional: Plot decision boundary for first two features (if interested)\n",
        "                                                   if X.shape[1] >= 2:\n",
        "                                                       plt.figure(figsize=(8, 6))\n",
        "                                                           plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap='coolwarm', alpha=0.6)\n",
        "                                                               plt.title('Decision Boundary (First Two Features)')\n",
        "                                                                   plt.xlabel(feature_names[0])\n",
        "                                                                       plt.ylabel(feature_names[1])\n",
        "                                                                           plt.show()"
      ],
      "metadata": {
        "id": "ymTV2v-mG6qk"
      },
      "id": "ymTV2v-mG6qk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "44. Write a Python program to train an SVM Regressor (SVR) and evaluate its performance using Mean Absolute\n",
        "Error (MAE) instead of MSE?\n"
      ],
      "metadata": {
        "id": "mGUa895aHO0U"
      },
      "id": "mGUa895aHO0U"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "import seaborn as sns\n",
        "\n",
        "# Load the California housing dataset\n",
        "data = fetch_california_housing()\n",
        "X = data.data\n",
        "y = data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Standardize features (important for SVR)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Initialize and train SVR model\n",
        "svr = SVR(kernel='rbf', C=1.0, epsilon=0.1)\n",
        "svr.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = svr.predict(X_test_scaled)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "# Print metrics\n",
        "print(\"SVR Performance Metrics:\")\n",
        "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
        "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
        "print(f\"R-squared (R²) Score: {r2:.4f}\")\n",
        "\n",
        "# Visualize predictions vs actual values\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(y_test, y_pred, alpha=0.5)\n",
        "plt.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=2)\n",
        "plt.xlabel('Actual Values')\n",
        "plt.ylabel('Predicted Values')\n",
        "plt.title('SVR: Actual vs Predicted Values')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Residual plot\n",
        "residuals = y_test - y_pred\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(y_pred, residuals, alpha=0.5)\n",
        "plt.axhline(y=0, color='r', linestyle='--')\n",
        "plt.xlabel('Predicted Values')\n",
        "plt.ylabel('Residuals')\n",
        "plt.title('Residual Plot')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Feature importance (for linear kernel)\n",
        "if svr.kernel == 'linear':\n",
        "    plt.figure(figsize=(10, 6))\n",
        "        importance = svr.coef_[0]\n",
        "            sorted_idx = np.argsort(np.abs(importance))[::-1]\n",
        "                plt.barh(range(X.shape[1]), importance[sorted_idx], align='center')\n",
        "                    plt.yticks(range(X.shape[1]), feature_names[sorted_idx])\n",
        "                        plt.xlabel('Coefficient Value')\n",
        "                            plt.title('Feature Importance (Linear SVR)')\n",
        "                                plt.tight_layout()\n",
        "                                    plt.show()"
      ],
      "metadata": {
        "id": "nz02gL6SHnK6"
      },
      "id": "nz02gL6SHnK6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "45. Write a Python program to train a Naïve Bayes classifier and evaluate its performance using the ROC-AUC\n",
        "score?"
      ],
      "metadata": {
        "id": "MYbzJBkTHpQX"
      },
      "id": "MYbzJBkTHpQX"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import seaborn as sns\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "target_names = data.target_names\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Standardize features (important for probability calibration)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Initialize and train Naïve Bayes classifier\n",
        "nb_classifier = GaussianNB()\n",
        "nb_classifier.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Get predicted probabilities for the positive class (class 1)\n",
        "y_pred_proba = nb_classifier.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "# Calculate ROC-AUC score\n",
        "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "\n",
        "# Compute ROC curve\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "# Plot ROC curve\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Guessing')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate (1 - Specificity)')\n",
        "plt.ylabel('True Positive Rate (Sensitivity)')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve\\nNaïve Bayes Classifier')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "# Print evaluation metrics\n",
        "print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, nb_classifier.predict(X_test_scaled), target_names=target_names))\n",
        "\n",
        "# Additional: Plot probability distributions\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(y_pred_proba[y_test == 0], bins=30, alpha=0.5, label='Class 0 (Benign)', color='blue')\n",
        "plt.hist(y_pred_proba[y_test == 1], bins=30, alpha=0.5, label='Class 1 (Malignant)', color='red')\n",
        "plt.xlabel('Predicted Probability for Class 1')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Distribution of Predicted Probabilities')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Confusion matrix with optimal threshold\n",
        "optimal_idx = np.argmax(tpr - fpr)\n",
        "optimal_threshold = thresholds[optimal_idx]\n",
        "y_pred_optimal = (y_pred_proba >= optimal_threshold).astype(int)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "cm = confusion_matrix(y_test, y_pred_optimal)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=target_names, yticklabels=target_names)\n",
        "            plt.xlabel('Predicted')\n",
        "            plt.ylabel('Actual')\n",
        "            plt.title(f'Confusion Matrix (Threshold = {optimal_threshold:.2f})')\n",
        "            plt.show()"
      ],
      "metadata": {
        "id": "2qG7ecuCHu43"
      },
      "id": "2qG7ecuCHu43",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "46. Write a Python program to train an SVM Classifier and visualize the Precision-Recall Curve?"
      ],
      "metadata": {
        "id": "Sk-mgd1XIH7d"
      },
      "id": "Sk-mgd1XIH7d"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import svm, datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the Breast Cancer dataset (binary classification)\n",
        "data = datasets.load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "target_names = data.target_names\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Standardize features (important for SVM)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Train SVM classifier\n",
        "svm_clf = svm.SVC(kernel='rbf', probability=True, random_state=42)\n",
        "svm_clf.fit(X_train, y_train)\n",
        "\n",
        "# Get predicted probabilities for the positive class\n",
        "y_scores = svm_clf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate precision-recall curve\n",
        "precision, recall, thresholds = precision_recall_curve(y_test, y_scores)\n",
        "average_precision = average_precision_score(y_test, y_scores)\n",
        "\n",
        "# Plot the Precision-Recall curve\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.step(recall, precision, where='post', color='b', alpha=0.8,\n",
        "         label=f'SVM (AP = {average_precision:.2f})')\n",
        "         plt.fill_between(recall, precision, step='post', alpha=0.2, color='b')\n",
        "\n",
        "         # Add baseline (random classifier)\n",
        "         baseline = len(y_test[y_test==1]) / len(y_test)\n",
        "         plt.axhline(y=baseline, color='r', linestyle='--',\n",
        "                     label=f'Baseline (AP = {baseline:.2f})')\n",
        "\n",
        "                     # Format the plot\n",
        "                     plt.xlabel('Recall (Sensitivity)', fontsize=12)\n",
        "                     plt.ylabel('Precision (Positive Predictive Value)', fontsize=12)\n",
        "                     plt.ylim([0.0, 1.05])\n",
        "                     plt.xlim([0.0, 1.0])\n",
        "                     plt.title('Precision-Recall Curve\\nSVM Classifier on Breast Cancer Dataset', fontsize=14)\n",
        "                     plt.legend(loc='upper right', fontsize=12)\n",
        "                     plt.grid(True, alpha=0.3)\n",
        "                     plt.tight_layout()\n",
        "                     plt.show()\n",
        "\n",
        "                     # Print the optimal threshold (maximizing F1-score)\n",
        "                     f1_scores = 2 * (precision * recall) / (precision + recall + 1e-8)\n",
        "                     optimal_idx = np.argmax(f1_scores)\n",
        "                     optimal_threshold = thresholds[optimal_idx]\n",
        "                     print(f\"Optimal Threshold: {optimal_threshold:.4f}\")\n",
        "                     print(f\"Optimal Precision: {precision[optimal_idx]:.4f}\")\n",
        "                     print(f\"Optimal Recall: {recall[optimal_idx]:.4f}\")\n",
        "                     print(f\"Optimal F1-score: {f1_scores[optimal_idx]:.4f}\")\n",
        "\n",
        "                     # Confusion matrix at optimal threshold\n",
        "                     from sklearn.metrics import confusion_matrix\n",
        "                     import seaborn as sns\n",
        "\n",
        "                     y_pred = (y_scores >= optimal_threshold).astype(int)\n",
        "                     cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "                     plt.figure(figsize=(8, 6))\n",
        "                     sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                                 xticklabels=target_names, yticklabels=target_names)\n",
        "                                 plt.xlabel('Predicted')\n",
        "                                 plt.ylabel('Actual')\n",
        "                                 plt.title(f'Confusion Matrix at Threshold = {optimal_threshold:.2f}')\n",
        "                                 plt.show()"
      ],
      "metadata": {
        "id": "xEerh92TIbwp"
      },
      "id": "xEerh92TIbwp",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "cell_execution_strategy": "setup",
      "generative_ai_disabled": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}